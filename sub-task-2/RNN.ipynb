{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "input_size = 84\n",
    "\n",
    "output_size = 1\n",
    "\n",
    "hidden_size = input_size*3\n",
    "\n",
    "layers = 3\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as thdata\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.layer1 = nn.RNN(input_size, hidden_size, layers).to(device='cuda')\n",
    "        self.fc = nn.Linear(hidden_size, output_size).to(device='cuda')\n",
    "        # self.h0 = torch.zeros(1, batch_size, hidden_size).to(device='cuda')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(layers, batch_size, hidden_size).to(device='cuda')\n",
    "        output, hn = self.layer1(x, h0)\n",
    "        y = self.fc(output[-1])\n",
    "        y = torch.sigmoid(y)\n",
    "        return y\n",
    "    \n",
    "class GRU(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(GRU, self).__init__()\n",
    "        self.layer1 = nn.GRU(input_size, hidden_size, layers).to(device='cuda')\n",
    "        self.fc = nn.Linear(hidden_size, output_size).to(device='cuda')\n",
    "        # self.h0 = torch.zeros(1, batch_size, hidden_size).to(device='cuda')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(layers, batch_size, hidden_size).to(device='cuda')\n",
    "        output, hn = self.layer1(x, h0)\n",
    "        y = self.fc(output[-1])\n",
    "        y = torch.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(LSTM, self).__init__()\n",
    "        self.layer1 = nn.LSTM(input_size, hidden_size, layers).to(device='cuda')\n",
    "        self.fc = nn.Linear(hidden_size, output_size).to(device='cuda')\n",
    "        # self.h0 = torch.zeros(1, batch_size, hidden_size).to(device='cuda')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(layers, batch_size, hidden_size).to(device='cuda')\n",
    "        c0 = torch.zeros(layers, batch_size, hidden_size).to(device='cuda')\n",
    "        output, _ = self.layer1(x, (h0, c0))\n",
    "        y = self.fc(output[-1])\n",
    "        y = torch.sigmoid(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LEM'''\n",
    "\n",
    "import math\n",
    "\n",
    "class LEMCell(nn.Module):\n",
    "    def __init__(self, ninp, nhid, dt):\n",
    "        super(LEMCell, self).__init__()\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.dt = dt\n",
    "        self.inp2hid = nn.Linear(ninp, 4 * nhid)\n",
    "        self.hid2hid = nn.Linear(nhid, 3 * nhid)\n",
    "        self.transform_z = nn.Linear(nhid, nhid)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.nhid)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        transformed_inp = self.inp2hid(x)\n",
    "        transformed_hid = self.hid2hid(y)\n",
    "        i_dt1, i_dt2, i_z, i_y = transformed_inp.chunk(4, 1)\n",
    "        h_dt1, h_dt2, h_y = transformed_hid.chunk(3, 1)\n",
    "\n",
    "        ms_dt_bar = self.dt * torch.sigmoid(i_dt1 + h_dt1)\n",
    "        ms_dt = self.dt * torch.sigmoid(i_dt2 + h_dt2)\n",
    "\n",
    "        z = (1.-ms_dt) * z + ms_dt * torch.tanh(i_y + h_y)\n",
    "        y = (1.-ms_dt_bar)* y + ms_dt_bar * torch.tanh(self.transform_z(z)+i_z)\n",
    "\n",
    "        return y, z\n",
    "\n",
    "class LEM(nn.Module):\n",
    "    def __init__(self, dt=1.):\n",
    "        super(LEM, self).__init__()\n",
    "        self.nhid = hidden_size\n",
    "        self.cell = LEMCell(input_size,hidden_size,dt)\n",
    "        self.classifier = nn.Linear(hidden_size, output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'classifier' in name and 'weight' in name:\n",
    "                nn.init.kaiming_normal_(param.data)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ## initialize hidden states\n",
    "        y = input.data.new(input.size(1), self.nhid).zero_()\n",
    "        z = input.data.new(input.size(1), self.nhid).zero_()\n",
    "        for x in input:\n",
    "            y, z = self.cell(x,y,z)\n",
    "        out = self.classifier(y)\n",
    "        out = nn.Sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MLP'''\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size*5, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred, label):\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i in range(batch_size):\n",
    "        if label[i] == 1:\n",
    "            if F.mse_loss(pred[i], torch.ones_like(pred[i])) < F.mse_loss(pred[i], torch.zeros_like(pred[i])):\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if F.mse_loss(pred[i], torch.ones_like(pred[i])) > F.mse_loss(pred[i], torch.zeros_like(pred[i])):\n",
    "                TN+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "    return TP, FP, TN, FN\n",
    "    \n",
    "    \n",
    "def test(dataloader, model, model_name=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        TP, FP, TN, FN = 0, 0, 0, 0\n",
    "        for data, label in dataloader:\n",
    "            if model_name=='mlp':\n",
    "                pred = model(Variable(data.reshape(batch_size, input_size*5).to(device='cuda')))\n",
    "            else:\n",
    "                pred = model(Variable(data.permute(1, 0, 2).to(device='cuda')))\n",
    "            tp, fp, tn, fn = metrics(pred, label)\n",
    "            TP += tp\n",
    "            FP += fp\n",
    "            TN += tn\n",
    "            FN += fn\n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"path\")\n",
    "models = ['rnn', 'mlp', 'gru', 'lstm']\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    dataset = pickle.load(open('path/{}'.format(file), 'rb'))\n",
    "\n",
    "    data = [item[0] for item in dataset]\n",
    "    label = torch.asarray([item[1] for item in dataset], dtype=torch.float)\n",
    "    data = np.reshape(data, (len(data), 5*84))\n",
    "\n",
    "    norm = torch.nn.BatchNorm1d(5*84, affine=True)\n",
    "    input = torch.asarray(data, dtype=torch.float)\n",
    "    output = norm(input).reshape(len(data), 5, 84)\n",
    "\n",
    "    dataset = thdata.TensorDataset(output, label)\n",
    "\n",
    "    train_dataset, test_dataset = thdata.random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "    train_dataset_loader = thdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    test_dataset_loader = thdata.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    for model_name in models:\n",
    "        result_file = open('result/{}_{}_{}_new'.format(file[:-4], model_name, batch_size), 'w+')\n",
    "        if model_name == 'rnn':\n",
    "            model = RNN().to(device='cuda')\n",
    "        if model_name == 'gru':\n",
    "            model = GRU().to(device='cuda')\n",
    "        if model_name == 'lstm':\n",
    "            model = LSTM().to(device='cuda')\n",
    "        if model_name == 'lem':\n",
    "            model = LEM().to(device='cuda')\n",
    "        if model_name == 'mlp':\n",
    "            model = MLP().to(device='cuda')\n",
    "            \n",
    "        opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        epoch = 50\n",
    "        \n",
    "        tqdm_epoch = tqdm.tqdm([i for i in range(epoch)], desc='{}-{}'.format(file[:-4], model_name))\n",
    "        for e in tqdm_epoch:\n",
    "            loss_sum = 0\n",
    "            for data, label in train_dataset_loader:\n",
    "                label = Variable(label.to(device='cuda'))\n",
    "                if model_name=='mlp':\n",
    "                    pred = model(Variable(data.reshape(batch_size, input_size*5).to(device='cuda')))\n",
    "                else:\n",
    "                    pred = model(Variable(data.permute(1, 0, 2).to(device='cuda')))\n",
    "                loss = F.mse_loss(pred, label)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                \n",
    "                loss_sum += loss.item()\n",
    "                \n",
    "            \n",
    "            model.eval()\n",
    "            TP, FP, TN, FN = test(test_dataset_loader, model, model_name)\n",
    "            P = TP / (TP + FP) if (TP + FP) else 0\n",
    "            R = TP / (TP + FN) if (TP + FN) else 0\n",
    "            F1 = 2 * P * R / (P + R) if (P + R) else 0\n",
    "            tqdm_epoch.set_postfix(loss='{}'.format(loss_sum / len(train_dataset_loader)), TPR=TPR, FPR=FPR, ACC=ACC, refresh=False)\n",
    "            model.train()\n",
    "                    \n",
    "            result_file.write('{}, {}, {}, {}, {}, {}, {}\\n'.format(e, loss_sum, TP, FP, TN, FN, ACC))\n",
    "            result_file.flush()\n",
    "        \n",
    "        result_file.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f381d70e597214683a7966ab2fd2893e10012246e6f9f6e78076ec4753f3d72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
